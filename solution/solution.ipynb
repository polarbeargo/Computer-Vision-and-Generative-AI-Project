{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9415a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gradio==3.50.2 in /home/student/.local/lib/python3.10/site-packages (3.50.2)\n",
      "Requirement already satisfied: ipywidgets==8.1.1 in /home/student/.local/lib/python3.10/site-packages (8.1.1)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /home/student/.local/lib/python3.10/site-packages (from gradio==3.50.2) (4.9.0)\n",
      "Requirement already satisfied: ffmpy in /home/student/.local/lib/python3.10/site-packages (from gradio==3.50.2) (0.3.1)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.50.2) (2.1.1)\n",
      "Requirement already satisfied: fastapi in /home/student/.local/lib/python3.10/site-packages (from gradio==3.50.2) (0.105.0)\n",
      "Requirement already satisfied: requests~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.50.2) (2.29.0)\n",
      "Requirement already satisfied: pydub in /home/student/.local/lib/python3.10/site-packages (from gradio==3.50.2) (0.25.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /home/student/.local/lib/python3.10/site-packages (from gradio==3.50.2) (0.25.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.50.2) (0.19.4)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /home/student/.local/lib/python3.10/site-packages (from gradio==3.50.2) (5.2.0)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.50.2) (9.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio==3.50.2) (23.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.50.2) (2.1.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /home/student/.local/lib/python3.10/site-packages (from gradio==3.50.2) (11.0.3)\n",
      "Requirement already satisfied: python-multipart in /home/student/.local/lib/python3.10/site-packages (from gradio==3.50.2) (0.0.6)\n",
      "Requirement already satisfied: gradio-client==0.6.1 in /home/student/.local/lib/python3.10/site-packages (from gradio==3.50.2) (0.6.1)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /home/student/.local/lib/python3.10/site-packages (from gradio==3.50.2) (23.2.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.50.2) (3.1.2)\n",
      "Requirement already satisfied: httpx in /home/student/.local/lib/python3.10/site-packages (from gradio==3.50.2) (0.26.0)\n",
      "Requirement already satisfied: orjson~=3.0 in /home/student/.local/lib/python3.10/site-packages (from gradio==3.50.2) (3.9.10)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /home/student/.local/lib/python3.10/site-packages (from gradio==3.50.2) (2.10.0)\n",
      "Requirement already satisfied: numpy~=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.50.2) (1.24.3)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.50.2) (6.0)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.50.2) (3.8.2)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /home/student/.local/lib/python3.10/site-packages (from gradio==3.50.2) (6.1.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /home/student/.local/lib/python3.10/site-packages (from gradio==3.50.2) (2.5.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from ipywidgets==8.1.1) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /home/student/.local/lib/python3.10/site-packages (from ipywidgets==8.1.1) (3.0.9)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets==8.1.1) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /home/student/.local/lib/python3.10/site-packages (from ipywidgets==8.1.1) (4.0.9)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets==8.1.1) (8.12.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==0.6.1->gradio==3.50.2) (2023.12.2)\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==3.50.2) (0.12.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==3.50.2) (4.20.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.14.0->gradio==3.50.2) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.14.0->gradio==3.50.2) (4.65.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.1.1) (0.7.5)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.1.1) (2.15.1)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.1.1) (5.1.1)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.1.1) (0.2.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.1.1) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.1.1) (0.1.6)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.1.1) (4.8.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.1.1) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.1.1) (3.0.36)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.50.2) (0.12.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.50.2) (1.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.50.2) (1.4.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.50.2) (4.46.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.50.2) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.50.2) (3.1.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio==3.50.2) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio==3.50.2) (2022.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/student/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.50.2) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /home/student/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.50.2) (2.14.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio==3.50.2) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio==3.50.2) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio==3.50.2) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio==3.50.2) (2.0.4)\n",
      "Requirement already satisfied: h11>=0.8 in /home/student/.local/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio==3.50.2) (0.14.0)\n",
      "Requirement already satisfied: click>=7.0 in /home/student/.local/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio==3.50.2) (8.1.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /home/student/.local/lib/python3.10/site-packages (from fastapi->gradio==3.50.2) (0.27.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /home/student/.local/lib/python3.10/site-packages (from fastapi->gradio==3.50.2) (3.7.1)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->gradio==3.50.2) (1.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/student/.local/lib/python3.10/site-packages (from httpx->gradio==3.50.2) (1.0.2)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio==3.50.2) (1.1.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets==8.1.1) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2) (23.1.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2) (0.32.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2) (0.13.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2) (2023.11.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets==8.1.1) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets==8.1.1) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==3.50.2) (1.16.0)\n",
      "Requirement already satisfied: executing in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.1) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.1) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.1) (0.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio==3.50.2 ipywidgets==8.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4742765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import SamModel, SamProcessor\n",
    "from diffusers import DiffusionPipeline, AutoPipelineForText2Image, AutoPipelineForInpainting\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e37037aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11af8e13ddf4ccdb0e46004727f676e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'decay': 0.9999, 'inv_gamma': 1.0, 'min_decay': 0.0, 'optimization_step': 37000, 'power': 0.6666666666666666, 'update_after_step': 0, 'use_ema_warmup': False} were passed to UNet2DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    }
   ],
   "source": [
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(\"cuda\")\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "pipeline = AutoPipelineForInpainting.from_pretrained(\n",
    "    \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\", torch_dtype=torch.float16\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "224b2928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_inputs(image, input_points):\n",
    "    \n",
    "    inputs = processor(\n",
    "       image, \n",
    "       input_points=input_points,\n",
    "       return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    masks = processor.image_processor.post_process_masks(\n",
    "       outputs.pred_masks.cpu(), \n",
    "       inputs[\"original_sizes\"].cpu(), \n",
    "       inputs[\"reshaped_input_sizes\"].cpu()\n",
    "    )\n",
    "    \n",
    "    best_mask = masks[0][0][outputs.iou_scores.argmax()]\n",
    "    \n",
    "    bg_transparent = np.zeros(best_mask.shape + (4, ), dtype=np.uint8)\n",
    "    bg_transparent[best_mask == 0] = [0, 255, 0, 127]  \n",
    "\n",
    "    \n",
    "    return bg_transparent\n",
    "\n",
    "\n",
    "def inpaint(raw_image, input_mask, prompt, negative_prompt=None, seed=74294536, cfgs=7):\n",
    "    \n",
    "    mask_image = Image.fromarray(input_mask)\n",
    "    rand_gen = torch.manual_seed(seed)\n",
    "    image = pipeline(\n",
    "        prompt=prompt, \n",
    "        negative_prompt=negative_prompt, \n",
    "        image=raw_image, \n",
    "        mask_image=mask_image, \n",
    "        generator=rand_gen, \n",
    "        guidance_scale=cfgs\n",
    "    ).images[0]\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7bb51e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 768.00 MiB (GPU 0; 14.76 GiB total capacity; 1009.84 MiB already allocated; 574.75 MiB free; 1.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m raw_image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(requests\u001b[38;5;241m.\u001b[39mget(img_url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mraw)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m))\n\u001b[1;32m      4\u001b[0m input_points \u001b[38;5;241m=\u001b[39m [[[\u001b[38;5;241m150\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m170\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m300\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m250\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m]]] \u001b[38;5;66;03m# 2D localization of a window\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mget_processed_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# prompt = \"a car stuck in traffic in New York on a rainy day\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# prompt = \"a car driving on the beach in broad daylight\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma car driving on the Mars. Studio lights, 1970s\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m, in \u001b[0;36mget_processed_inputs\u001b[0;34m(image, input_points)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_processed_inputs\u001b[39m(image, input_points):\n\u001b[1;32m      3\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m processor(\n\u001b[1;32m      4\u001b[0m        image, \n\u001b[1;32m      5\u001b[0m        input_points\u001b[38;5;241m=\u001b[39minput_points,\n\u001b[1;32m      6\u001b[0m        return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     masks \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mimage_processor\u001b[38;5;241m.\u001b[39mpost_process_masks(\n\u001b[1;32m     12\u001b[0m        outputs\u001b[38;5;241m.\u001b[39mpred_masks\u001b[38;5;241m.\u001b[39mcpu(), \n\u001b[1;32m     13\u001b[0m        inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu(), \n\u001b[1;32m     14\u001b[0m        inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreshaped_input_sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     15\u001b[0m     )\n\u001b[1;32m     17\u001b[0m     best_mask \u001b[38;5;241m=\u001b[39m masks[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][outputs\u001b[38;5;241m.\u001b[39miou_scores\u001b[38;5;241m.\u001b[39margmax()]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/sam/modeling_sam.py:1361\u001b[0m, in \u001b[0;36mSamModel.forward\u001b[0;34m(self, pixel_values, input_points, input_labels, input_boxes, input_masks, image_embeddings, multimask_output, attention_similarity, target_embedding, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m vision_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1361\u001b[0m     vision_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1367\u001b[0m     image_embeddings \u001b[38;5;241m=\u001b[39m vision_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/sam/modeling_sam.py:1050\u001b[0m, in \u001b[0;36mSamVisionEncoder.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1046\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1047\u001b[0m         hidden_states,\n\u001b[1;32m   1048\u001b[0m     )\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1050\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/sam/modeling_sam.py:943\u001b[0m, in \u001b[0;36mSamVisionLayer.forward\u001b[0;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m    940\u001b[0m     height, width \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], hidden_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    941\u001b[0m     hidden_states, padding_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_partition(hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[0;32m--> 943\u001b[0m hidden_states, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/sam/modeling_sam.py:843\u001b[0m, in \u001b[0;36mSamVisionAttention.forward\u001b[0;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;66;03m# q, k, v with shape (batch_size * nHead, height * width, channel)\u001b[39;00m\n\u001b[1;32m    841\u001b[0m query, key, value \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m3\u001b[39m, batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads, height \u001b[38;5;241m*\u001b[39m width, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 843\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_pos:\n\u001b[1;32m    846\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_decomposed_rel_pos(\n\u001b[1;32m    847\u001b[0m         attn_weights, query, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_pos_h, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_pos_w, (height, width), (height, width)\n\u001b[1;32m    848\u001b[0m     )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 768.00 MiB (GPU 0; 14.76 GiB total capacity; 1009.84 MiB already allocated; 574.75 MiB free; 1.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "img_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\").resize((1024, 1024))\n",
    "\n",
    "input_points = [[[150 * 2, 170 * 2], [300 * 2, 250 * 2]]] # 2D localization of a window\n",
    "\n",
    "mask = get_processed_inputs(raw_image, input_points)\n",
    "\n",
    "# prompt = \"a car stuck in traffic in New York on a rainy day\"\n",
    "# prompt = \"a car driving on the beach in broad daylight\"\n",
    "prompt = \"a car driving on the Mars. Studio lights, 1970s\"\n",
    "negative_prompt = \"artifacts, low quality, distortion\"\n",
    "\n",
    "image = inpaint(raw_image, mask, prompt, negative_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d87a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_image_grid([raw_image, Image.fromarray(mask), image], rows=1, cols=3)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d2dff7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "HEIGHT = 512\n",
    "WIDTH = 512\n",
    "\n",
    "input_points = []\n",
    "\n",
    "def get_points(img, evt: gr.SelectData):\n",
    "\n",
    "    x = evt.index[0]\n",
    "    y = evt.index[1]\n",
    "    \n",
    "    input_points.append([x, y])\n",
    "    \n",
    "    gr.Info(f\"Added point {x}, {y}\")\n",
    "\n",
    "\n",
    "def run_sam(raw_image):\n",
    "    global input_points\n",
    "    \n",
    "    if len(input_points) == 0:\n",
    "        raise gr.Error(\"No points provided. Click on the image to select the object to segment with SAM\")\n",
    "    \n",
    "    print(f\"Received {len(input_points)} points\")\n",
    "    print(\"Processing input image with SAM...\")\n",
    "    mask = get_processed_inputs(raw_image, [input_points])\n",
    "    \n",
    "    input_points = []\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "def run(raw_image, mask, prompt, negative_prompt, cfg, seed):\n",
    "    \n",
    "    # prompt = \"a car stuck in traffic in New York on a rainy day\"\n",
    "    # prompt = \"a car driving on the beach in broad daylight\"\n",
    "    # prompt = \"a car driving on the Mars. Studio lights, 1970s\"\n",
    "    # negative_prompt = \"artifacts, low quality, distortion\"\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Negative prompt: {negative_prompt}\")\n",
    "    \n",
    "    print(\"Inpainting...\")\n",
    "    inpainted = inpaint(raw_image, np.array(mask), prompt, negative_prompt, seed, cfg)\n",
    "    print(\"done!\")\n",
    "    return inpainted\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    \n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    # Background swap\n",
    "    1. Select an image by clicking on the first canvas\n",
    "    2. Click a few times *slowly* on different points of the subject \n",
    "       you want to keep. For example, in the Mona Lisa example, click multiple times on different areas of Mona \n",
    "       Lisa's skin.\n",
    "    3. Click on \"Run SAM\". This will show you the mask that will be used. If you don't like the mask, \n",
    "       retry again point 2 (note: points are NOT kept from the previous try)\n",
    "    4. Write a prompt (and optionally a negative prompt) for what you want to generate as the background of your \n",
    "       subject, then click on \"run inpaint\". If you are not happy with the result, change your prompts, the \n",
    "       settings (CFG scale, random seed) and click \"run inpaint\" again.\n",
    "    \n",
    "    > NOTE: the generation of the background can take up to a couple of minutes. Be patient!\n",
    "    \n",
    "    You can see a few examples scrolling down. Click on an example and everything will be prefilled for you. Note \n",
    "    however that you still need to click on the subject and go through the process \"run SAM\" -> \"run inpaint\".\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        input_img = gr.Image(\n",
    "            label=\"Input\", \n",
    "            interactive=True, \n",
    "            type='pil',\n",
    "            height=HEIGHT,\n",
    "            width=WIDTH\n",
    "        )\n",
    "        input_img.select(get_points, inputs=[input_img])\n",
    "                \n",
    "        sam_mask = gr.Image(\n",
    "            label=\"SAM result\",\n",
    "            interactive=False,\n",
    "            type='pil',\n",
    "            height=HEIGHT,\n",
    "            width=WIDTH,\n",
    "            elem_id=\"sam_mask\"\n",
    "        )\n",
    "                \n",
    "        result = gr.Image(\n",
    "            label=\"Output\",\n",
    "            interactive=False,\n",
    "            type='pil',\n",
    "            height=HEIGHT,\n",
    "            width=WIDTH,\n",
    "            elem_id=\"output_image\"\n",
    "        )\n",
    "    \n",
    "    with gr.Row():\n",
    "        cfg = gr.Slider(\n",
    "            label=\"Classifier-Free Guidance Scale\", minimum=0.0, maximum=20.0, value=7, step=0.05\n",
    "        )\n",
    "        random_seed = gr.Number(\n",
    "            label=\"Random seed\", \n",
    "            value=74294536, \n",
    "            precision=0\n",
    "        )\n",
    "    \n",
    "    with gr.Row():\n",
    "        prompt = gr.Textbox(\n",
    "            label=\"Prompt for infill\"\n",
    "        )\n",
    "        neg_prompt = gr.Textbox(\n",
    "            label=\"Negative prompt\"\n",
    "        )\n",
    "        submit_sam = gr.Button(value=\"run SAM\")\n",
    "        submit_inpaint = gr.Button(value=\"run inpaint\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        examples = gr.Examples(\n",
    "            [\n",
    "                [\n",
    "                    \"car.png\", \n",
    "                    \"a car driving on the Mars. Studio lights, 1970s\", \n",
    "                    \"artifacts, low quality, distortion\",\n",
    "                    74294536\n",
    "                ],\n",
    "                [\n",
    "                    \"dragon.jpeg\",\n",
    "                    \"a dragon in a medieval village\",\n",
    "                    \"artifacts, low quality, distortion\",\n",
    "                    97\n",
    "                ],\n",
    "                [\n",
    "                    \"monalisa.png\",\n",
    "                    \"a fantasy landscape with flying dragons\",\n",
    "                    \"artifacts, low quality, distortion\",\n",
    "                    97\n",
    "                ]\n",
    "            ],\n",
    "            inputs=[\n",
    "                input_img,\n",
    "                prompt,\n",
    "                neg_prompt,\n",
    "                random_seed\n",
    "            ]\n",
    "            \n",
    "        )\n",
    "    \n",
    "    submit_sam.click(\n",
    "        fn=run_sam,\n",
    "        inputs=[input_img],\n",
    "        outputs=[sam_mask]\n",
    "    )\n",
    "    \n",
    "    submit_inpaint.click(\n",
    "        fn=run, \n",
    "        inputs=[\n",
    "            input_img, \n",
    "            sam_mask,\n",
    "            prompt, \n",
    "            neg_prompt,\n",
    "            cfg,\n",
    "            random_seed\n",
    "        ], \n",
    "        outputs=[result]\n",
    "    )\n",
    "\n",
    "demo.queue().launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b4dd12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
